# -*- coding: utf-8 -*-
"""scrap.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uYR7TBHZXELoaS8lTii1mSZPEH0FxmG1
"""

pip install beautifulsoup4

pip install requests

"""URSULA CORBERO - TOKYO"""

# Import packages
from urllib.request import urlopen
from bs4 import BeautifulSoup
# Specify url of the web page
source = urlopen('https://it.wikipedia.org/wiki/%C3%9Arsula_Corber%C3%B3').read()
# Make a soup 
soup = BeautifulSoup(source,'html')
soup

soup = BeautifulSoup(source, 'html.parser')

print(soup.prettify())

# Extract the plain text content from paragraphs
text = ''
for paragraph in soup.find_all('p'):
    text += paragraph.text
    
text

# Import package
import re
# Clean text
text = re.sub(r'\[.*?\]+', '', text)
text0 = text.replace('\n', '')
text0

import pandas as pd

"""MIQUEL GARCIA BORDA - VICUNA"""

# Specify url of the web page
source = urlopen('https://it.wikipedia.org/wiki/Miquel_Garc%C3%ADa_Bord%C3%A1').read()
# Make a soup 
soup = BeautifulSoup(source,'html')
soup = BeautifulSoup(source, 'html.parser')
# Extract the plain text content from paragraphs
text = ''
for paragraph in soup.find_all('p'):
    text += paragraph.text
# Clean text
text = re.sub(r'\[.*?\]+', '', text)
text1 = text.replace('\n', '')
text1

"""ESTHER ACERBO - STOCCOLMA"""

# Specify url of the web page
source = urlopen('https://it.wikipedia.org/wiki/Esther_Acebo').read()
# Make a soup 
soup = BeautifulSoup(source,'html')
soup = BeautifulSoup(source, 'html.parser')
# Extract the plain text content from paragraphs
text = ''
for paragraph in soup.find_all('p'):
    text += paragraph.text
# Clean text
text = re.sub(r'\[.*?\]+', '', text)
text2 = text.replace('\n', '')
text2

"""NAJWA NIMRI - SIERRA"""

# Specify url of the web page
source = urlopen('https://it.wikipedia.org/wiki/Najwa_Nimri').read()
# Make a soup 
soup = BeautifulSoup(source,'html')
soup = BeautifulSoup(source, 'html.parser')
# Extract the plain text content from paragraphs
text = ''
for paragraph in soup.find_all('p'):
    text += paragraph.text
# Clean text
text = re.sub(r'\[.*?\]+', '', text)
text3 = text.replace('\n', '')
text3

"""MIGUEL HERRAN - RIO"""

# Specify url of the web page
source = urlopen('https://it.wikipedia.org/wiki/Miguel_Herr%C3%A1n').read()
# Make a soup 
soup = BeautifulSoup(source,'html')
soup = BeautifulSoup(source, 'html.parser')
# Extract the plain text content from paragraphs
text = ''
for paragraph in soup.find_all('p'):
    text += paragraph.text
# Clean text
text = re.sub(r'\[.*?\]+', '', text)
text4 = text.replace('\n', '')
text4

"""ALVARO MORTE - IL PROFESSORE"""

# Specify url of the web page
source = urlopen('https://it.wikipedia.org/wiki/%C3%81lvaro_Morte').read()
# Make a soup 
soup = BeautifulSoup(source,'html')
soup = BeautifulSoup(source, 'html.parser')
# Extract the plain text content from paragraphs
text = ''
for paragraph in soup.find_all('p'):
    text += paragraph.text
# Clean text
text = re.sub(r'\[.*?\]+', '', text)
text5 = text.replace('\n', '')
text5

"""JUAN FERNANDEZ - PRIETO"""

# Specify url of the web page
source = urlopen('https://it.wikipedia.org/wiki/Juan_Fern%C3%A1ndez_(attore)').read()
# Make a soup 
soup = BeautifulSoup(source,'html')
soup = BeautifulSoup(source, 'html.parser')
# Extract the plain text content from paragraphs
text = ''
for paragraph in soup.find_all('p'):
    text += paragraph.text
# Clean text
text = re.sub(r'\[.*?\]+', '', text)
text6 = text.replace('\n', '')
text6

"""RODRIGO DE LA SERNA - PALERMO"""

# Specify url of the web page
source = urlopen('https://it.wikipedia.org/wiki/Rodrigo_de_la_Serna').read()
# Make a soup 
soup = BeautifulSoup(source,'html')
soup = BeautifulSoup(source, 'html.parser')
# Extract the plain text content from paragraphs
text = ''
for paragraph in soup.find_all('p'):
    text += paragraph.text
# Clean text
text = re.sub(r'\[.*?\]+', '', text)
text7 = text.replace('\n', '')
text7

"""ALBA FLORES - NAIROBI"""

# Specify url of the web page
source = urlopen('https://it.wikipedia.org/wiki/Alba_Flores').read()
# Make a soup 
soup = BeautifulSoup(source,'html')
soup = BeautifulSoup(source, 'html.parser')
# Extract the plain text content from paragraphs
text = ''
for paragraph in soup.find_all('p'):
    text += paragraph.text
# Clean text
text = re.sub(r'\[.*?\]+', '', text)
text8 = text.replace('\n', '')
text8

"""PACO TOUS - MOSCA"""

# Specify url of the web page
source = urlopen('https://it.wikipedia.org/wiki/Paco_Tous').read()
# Make a soup 
soup = BeautifulSoup(source,'html')
soup = BeautifulSoup(source, 'html.parser')
# Extract the plain text content from paragraphs
text = ''
for paragraph in soup.find_all('p'):
    text += paragraph.text
# Clean text
text = re.sub(r'\[.*?\]+', '', text)
text9 = text.replace('\n', '')
text9

"""LUKA PEROS - MARSIGLIA"""

# Specify url of the web page
source = urlopen('https://it.wikipedia.org/wiki/Luka_Pero%C5%A1').read()
# Make a soup 
soup = BeautifulSoup(source,'html')
soup = BeautifulSoup(source, 'html.parser')
# Extract the plain text content from paragraphs
text = ''
for paragraph in soup.find_all('p'):
    text += paragraph.text
# Clean text
text = re.sub(r'\[.*?\]+', '', text)
text10 = text.replace('\n', '')
text10

"""BELEN CUESTA - MANILA"""

# Specify url of the web page
source = urlopen('https://it.wikipedia.org/wiki/Bel%C3%A9n_Cuesta').read()
# Make a soup 
soup = BeautifulSoup(source,'html')
soup = BeautifulSoup(source, 'html.parser')
# Extract the plain text content from paragraphs
text = ''
for paragraph in soup.find_all('p'):
    text += paragraph.text
# Clean text
text = re.sub(r'\[.*?\]+', '', text)
text11 = text.replace('\n', '')
text11

"""ITZIAR ITUNO - LISBONA"""

# Specify url of the web page
source = urlopen('https://it.wikipedia.org/wiki/Itziar_Itu%C3%B1o').read()
# Make a soup 
soup = BeautifulSoup(source,'html')
soup = BeautifulSoup(source, 'html.parser')
# Extract the plain text content from paragraphs
text = ''
for paragraph in soup.find_all('p'):
    text += paragraph.text
# Clean text
text = re.sub(r'\[.*?\]+', '', text)
text12 = text.replace('\n', '')
text12

"""DARKO PERIC - HELSINKI"""

# Specify url of the web page
source = urlopen('https://it.wikipedia.org/wiki/Darko_Peri%C4%87').read()
# Make a soup 
soup = BeautifulSoup(source,'html')
soup = BeautifulSoup(source, 'html.parser')
# Extract the plain text content from paragraphs
text = ''
for paragraph in soup.find_all('p'):
    text += paragraph.text
# Clean text
text = re.sub(r'\[.*?\]+', '', text)
text13 = text.replace('\n', '')
text13

"""KITI MANVER - FUENTES"""

# Specify url of the web page
source = urlopen('https://it.wikipedia.org/wiki/Kiti_M%C3%A1nver').read()
# Make a soup 
soup = BeautifulSoup(source,'html')
soup = BeautifulSoup(source, 'html.parser')
# Extract the plain text content from paragraphs
text = ''
for paragraph in soup.find_all('p'):
    text += paragraph.text
# Clean text
text = re.sub(r'\[.*?\]+', '', text)
text14 = text.replace('\n', '')
text14

"""JAIME LORENTE - DENVER"""

# Specify url of the web page
source = urlopen('https://it.wikipedia.org/wiki/Jaime_Lorente').read()
# Make a soup 
soup = BeautifulSoup(source,'html')
soup = BeautifulSoup(source, 'html.parser')
# Extract the plain text content from paragraphs
text = ''
for paragraph in soup.find_all('p'):
    text += paragraph.text
# Clean text
text = re.sub(r'\[.*?\]+', '', text)
text15 = text.replace('\n', '')
text15

"""HOVIK KEUCHKERIAN - BOGOTA"""

# Specify url of the web page
source = urlopen('https://it.wikipedia.org/wiki/Hovik_Keuchkerian').read()
# Make a soup 
soup = BeautifulSoup(source,'html')
soup = BeautifulSoup(source, 'html.parser')
# Extract the plain text content from paragraphs
text = ''
for paragraph in soup.find_all('p'):
    text += paragraph.text
# Clean text
text = re.sub(r'\[.*?\]+', '', text)
text16 = text.replace('\n', '')
text16

"""PEDRO ALONSO - BERLINO"""

# Specify url of the web page
source = urlopen('https://it.wikipedia.org/wiki/Pedro_Alonso_(attore)').read()
# Make a soup 
soup = BeautifulSoup(source,'html')
soup = BeautifulSoup(source, 'html.parser')
# Extract the plain text content from paragraphs
text = ''
for paragraph in soup.find_all('p'):
    text += paragraph.text
# Clean text
text = re.sub(r'\[.*?\]+', '', text)
text17 = text.replace('\n', '')
text17

"""ENRIQUE ARCE - ARTURO"""

# Specify url of the web page
source = urlopen('https://it.wikipedia.org/wiki/Enrique_Arce').read()
# Make a soup 
soup = BeautifulSoup(source,'html')
soup = BeautifulSoup(source, 'html.parser')
# Extract the plain text content from paragraphs
text = ''
for paragraph in soup.find_all('p'):
    text += paragraph.text
# Clean text
text = re.sub(r'\[.*?\]+', '', text)
text18 = text.replace('\n', '')
text18

"""CLARA ALVARADO - ARIADNA"""

# Specify url of the web page
source = urlopen('https://it.wikipedia.org/wiki/Clara_Alvarado').read()
# Make a soup 
soup = BeautifulSoup(source,'html')
soup = BeautifulSoup(source, 'html.parser')
# Extract the plain text content from paragraphs
text = ''
for paragraph in soup.find_all('p'):
    text += paragraph.text
# Clean text
text = re.sub(r'\[.*?\]+', '', text)
text19 = text.replace('\n', '')
text19

"""MARIA PEDRAZA - ALISON"""

# Specify url of the web page
source = urlopen('https://it.wikipedia.org/wiki/Mar%C3%ADa_Pedraza').read()
# Make a soup 
soup = BeautifulSoup(source,'html')
soup = BeautifulSoup(source, 'html.parser')
# Extract the plain text content from paragraphs
text = ''
for paragraph in soup.find_all('p'):
    text += paragraph.text
# Clean text
text = re.sub(r'\[.*?\]+', '', text)
text20 = text.replace('\n', '')
text20

data = {'Attore':  ['Ursula Corbero', 'Miquel Garcia Borda', 'Esther Acerbo', 'Najwa Nimri', 'Miguel Herran', 'Alvaro Morte', 'Juan Fernandez', 'Rodrigo de la Serna', 'Alba Flores', 'Paco Tous', 'Luka Peros', 'Belen Cuesta', 'Itziar Ituno', 'Darko Peric', 'Kiti Manvar', 'Jaime Lorente', 'Hovik Keuchkerian', 'Pedro Alonso', 'Enrique Arce', 'Clara Alvarado', 'Maria Pedraza'],
        'Biografia': [text0, text1, text2, text3, text4, text5, text6, text7, text8, text9, text10, text11, text12, text13, text14, text15, text16, text17, text18, text19, text20]}

df = pd.DataFrame (data, columns = ['Attore','Biografia'])

print (df)

df.to_csv('attore',index=False)

df = pd.read_csv("attore.csv", sep=',')
df.head()

df

df.head()